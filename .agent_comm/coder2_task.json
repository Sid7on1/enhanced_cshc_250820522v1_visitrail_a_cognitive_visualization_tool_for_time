{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "augmentation.py",
      "purpose": "Data augmentation techniques",
      "priority": "medium"
    },
    {
      "name": "feature_extraction.py",
      "purpose": "Feature extraction layers",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.HC_2508.20522v1_VisiTrail_A_Cognitive_Visualization_Tool_for_Time",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.HC_2508.20522v1_VisiTrail-A-Cognitive-Visualization-Tool-for-Time with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "I-Vt",
      "Data",
      "Calibration",
      "Supporting",
      "Learning",
      "Tive",
      "Fixation",
      "Temporal",
      "Data-Driven",
      "Personalize"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.HC_2508.20522v1_VisiTrail-A-Cognitive-Visualization-Tool-for-Time.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nARTICLE TEMPLATE\nVisiTrail: A Cognitive Visualization Tool for Time-Series\nAnalysis of Eye Tracking Data from Attention Game\nAbdul Rehmana, Ilona Heldala, and Jerry Chun-Wei Lina\naDepartment of Computer Science, Electrical Engineering and Mathematical Sciences,\nWestern Norway University of Applied Sciences, Bergen, Norway\nARTICLE HISTORY\nCompiled August 29, 2025\nABSTRACT\nEye Tracking (ET) can help to understand visual attention and cognitive processes\nin interactive environments. In attention tasks, distinguishing between relevant tar-\nget objects and distractors is crucial for effective performance, yet the underlying\ngaze patterns that drive successful task completion remain incompletely understood.\nTraditional gaze analyses lack comprehensive insights into the temporal dynamics\nof attention allocation and the relationship between gaze behavior and task perfor-\nmance. When applied to complex visual search scenarios, current gaze analysis meth-\nods face several limitations, including the isolation of measurements, visual stability,\nsearch efficiency, and the decision-making processes involved in these scenarios. This\npaper proposes an analysis tool that considers time series for eye tracking data from\ntask performance and also gaze measures (fixations, saccades and smooth pursuit);\ntemporal pattern analysis that reveals how attention evolves throughout task per-\nformance; object-click sequence tracking that directly links visual attention to user\nactions; and performance metrics that quantify both accuracy and efficiency. This\ntool provides comprehensive visualization techniques that make complex patterns\nof stimuli and gaze connections interpretable.\nKEYWORDS\nEye-tracking Technology; Visual Cognition; Cognitive Processing; Time Series\nAnalysis; Educational Technology; Visual Attention; Learning Analytics; Behavior\nAnalysis\n1. Introduction\nEye tracking is a widely utilized technology for analyzing how individuals visually in-\nteract with digital environments through gaze measures [1\u20133]. By calculating where and\nfor how long someone focuses on different areas of a screen, one can gain valuable in-\nsights into attention, decision-making processes, and cognitive load [4]. Recently, using\nET technology has seen significant application in serious game-based scenarios, where\nusers engage dynamically with evolving visual representations [5]. Unlike static images,\ngames invite variation in visual attention during gameplay [5], which can be recorded\nand measured by gaze data collected by ET. Analyzing gaze changes over time, along\nwith the progression of games and interactions, is referred to as temporal analysis [6].\nIt can enable us to determine not just the areas where people direct their gaze but also\napproximate attention transitions throughout the gaming experience [7].\nCONTACT: Abdul Rehman. Email: arj@hvl.noarXiv:2508.20522v1  [cs.HC]  28 Aug 2025\n\n--- Page 2 ---\nExamining gaze behavior changes within a temporal framework along with fixation\npatterns, saccadic movements, and response times from interactive tasks in serious\ngames offers deep insights that can enable (educators and psychologists do not manage\nthis yet) to quantify attention deficits, evaluate the effectiveness of the intervention, and\npersonalize learning experiences based on individual cognitive profiles [8]. For instance,\nidentifying how quickly a player notices critical game elements or how their focus adapts\nto new challenges can inform both game development and the evaluation of gameplay.\nConventional gaze analysis often emphasizes aggregated data such as heatmaps (in-\ntensity and frequency of fixation), which may overlook vital time-sensitive dynamics.\nAdopting a chronological perspective enables researchers to uncover patterns, such as\nhesitation, learning curves, or shifts in focal areas, which static summaries often fail\nto capture. This form of analysis is particularly beneficial in serious games or training\nsimulations, where tracking attention flow could help to evaluate performance or en-\ngagement levels with greater precision and depth. To overcome the challenges above,\nthis paper makes the following contributions:\n\u2022We propose a comprehensive cognitive visualization tool named VisiTrail for time-\nseries analysis tool that integrates multiple analytical dimensions: enhanced move-\nment classification that distinguishes between fixations, saccades, and smooth pur-\nsuit; temporal pattern analysis that reveals how attention evolves throughout task\nperformance; object-click sequence tracking that directly links visual attention to\nuser actions; and performance metrics that quantify both accuracy and efficiency\nof visual search behavior.\n\u2022We provide a comprehensive tool that makes complex patterns interpretable to\nresearchers and practitioners, allowing them to create personalized strategies that\nalign with the user\u2019s natural attention rhythms.\n\u2022The source code and demonstration video of VisiTrail are available1\nThe rest of the paper is organized as follows. Section 2 discusses related work on\neye tracking and learning environments. Section 3 presents our proposed approach for\nenhancing the Understanding of Time-Series Analysis of Eye-Tracking Data in interac-\ntive learning systems. Section 4 describes the experimental setup and results, including\nprivacy and performance evaluation. Finally, Section 5 concludes the paper and outlines\npossible directions for future research.\n2. Related Work\nFrutos-Pascual et al. [9] utilized ET technology to identify children\u2019s behavior in\nattention-enhancement therapies. By analyzing eye movement patterns during interac-\ntion with puzzle games, the researchers found that participants with better performance\nexhibited quantifiably different eye movement patterns compared to those with poorer\nresults. Piazzalunga et al. [10] used serious games-based ET data to identify children\nat risk of dysgraphia. By analyzing scan paths and fixation patterns during gameplay,\nthe study found that children with poorer performance had chaotic scan paths. In con-\ntrast, those who performed better had more ordered scan paths. The integration of\nET metrics with game performance data provided a nuanced understanding of visual\nperception impairments. Velichkovsky et al. [11] analyzed the distributions of fixation\ndurations among professional, amateur, and novice eSports players. They found that\n1https://github.com/arnor-git/VisiTrail\n2\n\n--- Page 3 ---\nhighly skilled gamers exhibit more variability in fixation durations and bimodal distri-\nbution patterns, indicating the presence of both ambient and focal fixation types.\nKim et al. [12] aimed to develop a deep learning (DL) model to identify individ-\nuals with mental illnesses who have impaired visuospatial memory encoding. During\na 3-minute memorization test of the RCFT, eye movements were recorded to evalu-\nate the structure and retention of psychosis, obsessive-compulsive disorder (OCD), and\nresponses from healthy controls. The resulting scores and fixation points, which show\nareas of eye focus, were used to create a Long Short-Term Memory (LSTM) model with\nan attention mechanism designed to distinguish between normal and impaired execu-\ntive function. Argasinski et al. [13] provided a framework for developing and evaluating\nserious games that incorporate ET and biosensor data, suggesting that the inclusion\nof temporal data is better for assessing user interactions and affective responses dur-\ning gameplay. Hajari et al. [14] explored team cognition by analyzing spatio-temporal\nET data during laparoscopic simulation operations. Using Cross Recurrence Analysis\n(CRA) and overlap analysis, they identified features that distinguish between high-\nand low-performing teams based on temporal gaze data patterns, thereby enhancing\nunderstanding of collaborative performance.\nDu et al. [15] introduced PrivateGaze, a solution that protects users\u2019 private informa-\ntion while interacting with black-box gaze tracking data, all while maintaining estima-\ntion accuracy. PrivateGaze effectively preserves sensitive user data, such as identity and\ngender, as demonstrated through experiments conducted on four benchmark datasets.\nPaskovske et al. [16] utilized ET to analyze how participants distribute their attention.\nTheir findings revealed a notable difference between novices and experts based on the\nnumber of fixations. Experts tend to spend less time on tasks and employ more efficient\nproblem-solving strategies. Additionally, the prevalence of NDDs has been increasing\nglobally, leading to a greater number of children with disabilities being integrated into\nmainstream schools. This shift calls for equitable and appropriate treatment of learners\nwith disabilities, driven by the implementation of educational policies and practices that\nsupport inclusive education.\nIn the past, Costescu et al. [5] primarily focused on developing a platform to enhance\nchildren\u2019s play experiences. A key concept of this platform is identifying the focus of\nchildren\u2019s attention during play activities. This can be examined through the game\n\u201dMushroom Hunter,\u201d which assesses the ability to sustain attention. Bueno et al. [17]\nfocused on developing an application specifically designed for AI research in education,\nparticularly for children with NDD. Thill et al., [18] investigated how robotics and social\nAI interpret children\u2019s behavior. Additionally, research by other studies [1,4] explored\nhow serious games, combined with eye-tracking technology, can provide insights that\nhelp teachers better support children with NDD. The study [19] proposed here builds on\nresearch into the development of platforms designed to assist children with NDD. This\nstudy focuses on providing a time-series analysis tool that integrates multiple analytical\ndimensions. It includes enhanced movement classification, which distinguishes between\nfixations, saccades, and smooth pursuit. The tool also offers temporal pattern analysis to\nreveal how attention evolves during task performance. Additionally, it features object-\nclick sequence tracking that directly links visual attention to user actions. Performance\nmetrics quantify both the accuracy and efficiency of visual search behavior, making\ncomplex patterns interpretable to researchers and practitioners. This allows them to\ndevelop personalized strategies that align with the user\u2019s natural attention rhythms.\n3\n\n--- Page 4 ---\n3. VisiTrail (Proposed Tool)\nFig. 1 provides an overview of the proposed tool from data collection, processing, quality\nvalidation, parameter selection, gaze classification, and performance metrics for two sce-\nnarios: a first-level overall analysis to provide detailed insights and a multilevel analysis\nto compare performance across all three levels.\nFigure 1. Proposed VisiTrail tool for Time-Series Analysis of Attention Game based Eye-Tracking Data\nAlgorithm 1 implements a comprehensive eye-tracking analysis pipeline designed to\nenhance the understanding of time-series analysis of eye-tracking data. The process\nbegins by parsing coordinate strings from CSV format, followed by the removal of in-\ncomplete records and validation of gaze positions within screen boundaries, using a\ntolerance of 50 pixels. Next, gaze behavior is classified using velocity-based thresholds.\nWe select a threshold of 721 px/s that differentiates fixation, indicating focused atten-\ntion, from saccades, which are rapid eye movements occurring during visual searches\ncalculated by using the I-VT algorithm [20,21]. Following this, the algorithm employs\ngreedy target-click matching with temporal constraints. A minimum reaction time of\n522 ms helps prevent false matches from anticipatory responses, while a maximum time\nof 5000 ms ensures that response windows remain realistic. The algorithm quantifies\ntask performance through several metrics: the hit rate (indicating target detection ac-\ncuracy), the false alarm rate (reflecting attention control), and the average reaction\ntime (measuring processing speed). It also computes screen utilization and gaze path\nlength to assess visual search strategies and patterns of attention distribution. Finally,\nthe algorithm generates evidence-based recommendations for educators and psycholo-\ngists based on validated performance thresholds derived from developmental and clinical\n4\n\n--- Page 5 ---\nguidelines.\nAlgorithm 1 Algorithm for Time-Series Analysis of Attention Game based Eye-\nTracking Data (VisiTrail Tool)\n1:Input: Mushroom Game (Attention) Dataset\n2:Parameters: Vthresh = 721 px/s, RTmin= 522 ms, RTmax= 5000 ms\n3:Pre-processing: Parse coordinates, remove missing data, filter valid screen bounds\n4:I-VT Fixation Detection:\n5:fori= 2 to ndo\n6:vmag,i\u2190\u221a\n(xi\u2212xi\u22121)2+(yi\u2212yi\u22121)2\n(Timestamp i\u2212Timestamp i\u22121)/1000\n7:movement type i\u2190fixation if vmag,i\u2264Vthresh , else saccade\n8:end for\n9:Event Extraction: Extract appear/disappear events and clicks, sort by timestamp\n10:Response Matching:\n11:Ttargets\u2190target appear events, Ccorrect\u2190correct clicks\n12:Initialize matched pairs\u21900,used clicks\u2190\u2205\n13:foreach target t\u2208Ttargets do\n14: Find earliest unused click cwhere t.time < c.time andc /\u2208used clicks\n15: rt\u2190c.timestamp\u2212t.timestamp\n16: ifRTmin\u2264rt\u2264RTmaxthen\n17: matched pairs\u2190matched pairs + 1,used clicks\u2190used clicks\u222a{c}\n18: end if\n19:end for\n20:Metrics: Compute hit rate, false alarm rate, reaction times, screen utilization\n21:Recommendations: Generate Recommendations based on performance thresholds\n22:Output: Performance metrics, spatial metrics, recommendations\n3.1. Dataset Selection, Pre-Processing and Data Quality Validation\nCostescu et al. [5] created and structured the sustained attention game that was used to\ngather data. It implements the Computerized Continuous Performance Task (CCPT)\naccording to the original task design created by [22]. Participants were shown a forest\nroad with flowers ( distractors ) and mushrooms ( targets ) sporadically appearing on either\nside. The goal is to avoid interacting with distractions and touch the screen when\na target appears. Data was gathered using a Tobii Pro Nano eye-tracker2, and the\ngame utilizes a head-positioning and calibration screen specifically designed for the\ntarget demographic. We specifically use Pilot 3 data collected from Romania, which\nincludes ET data from 8 students at a special education school in Romania [5]. The\nfirst step in the eye-tracking scoring system is to clean and prepare raw input data\nfor difficulty levels 1, 2, and 3. The system begins by loading CSV files to extract eye\nposition coordinates, which are initially stored as text strings (e.g., \u201d(1250, 680)\u201d). These\nstrings are converted into numerical values for analysis, and object positions and area-\nof-interest boundaries are extracted similarly. Next, the data is cleaned by removing\n2https://connect.tobii.com/s/article/how-to-configure-your-tobii-pro-nano-x2-or-x3-eye-tracker-with-the-mobile-device-stand?\nlanguage=en_US\n5\n\n--- Page 6 ---\ninvalid entries, such as records where the eye tracker detected zero coordinates or where\ntracking was inaccurate. Incomplete records that could cause errors in analysis are\nalso filtered out. The preprocessing phase adds necessary metadata, including student\nlabels, difficulty levels, and normalized timestamps, to ensure proper sequencing and\ndata integrity. Finally, the three-level files are combined into one comprehensive dataset.\nAfter preprocessing, the cleaned dataset contains valid eye-tracking coordinates, game\ninteraction events, and correctly labeled metadata, all of which are ready for detailed\nanalysis.\nIn the data quality validation step, we verified that the ET data was accurate and\nremoved any incorrect information. Eye trackers sometimes record impossible locations,\nsuch as negative coordinates or positions far outside the computer screen, so we needed\nto clean this up before analyzing the data. We further removed the rows that had missing\nET data. There were 304 Initial instances, and 30 of them were removed.\n3.2. Parameter Validation Process\nThe parameter validation level represents the methodological basis of this ET analysis\ntool, employing a sophisticated, data-driven approach that fundamentally departs from\nconventional assumption-based methodologies typically used in cognitive assessment re-\nsearch. We focus on four levels, especially reaction time analysis, velocity distribution,\ncoordinate quality and the area of interaction dimension. For reaction time, we mea-\nsured the time it took for students to respond to an object\u2019s appearance by recording\nthe moment the object appeared and when the student clicked on it. We found that\nmost responses occurred between 522 milliseconds and 5000 milliseconds, so we used\nthese as our minimum and maximum reaction times. Anything faster was likely an ac-\ncident, and anything slower indicated that the student was not paying attention. For\nvelocity distribution analysis, we measured the distance between the student\u2019s eyes at\none moment and the next, then divided by the time difference to calculate the speed\n[20,23]. We tested different speed limits and found that 721 pixels per second was the\nbest cutoff. Any value lower meant the eyes were staying still (fixation), while anything\nfaster meant the eyes were jumping around (saccade) [24]. Our threshold selection was\nnot arbitrary but based on systematic empirical validation of our eye-tracking data.\nThe threshold for classifying fixations was set at 721 pixels per second, based on the\n75th percentile of the gaze velocity distribution after removing outliers, as illustrated\nin Fig. 2. This threshold indicates that 75% of the recorded gaze velocities fall below\nthis value, allowing us to conservatively identify slower movements as fixations, while\nfiltering out faster movements, which are likely saccades. The analysis revealed that\ncommonly used lower thresholds, such as 30 pixels per second, captured only a small\nfraction of the data (approximately 4.2%) as fixations, significantly underestimating the\nactual number of fixations in this attention game dataset. By using the 75th percentile\nvalue, we align the threshold with the natural distribution of the data, enhancing the\nrealism and accuracy of fixation detection in the context of IVT analysis. The resulting\nvelocity histogram and fixation rates support this choice, demonstrating that 721 pixels\nper second effectively distinguishes fixations from rapid eye movements in this specific\ndataset.\nFor coordinate quality analysis, we tested the strictness of ET accuracy by examining\nthe number of data points that fell outside the screen boundaries. We then tried different\ntolerance levels to determine which one retained the most accurate data while removing\nobvious mistakes [25]. The 50-pixel coordinate tolerance strikes a balance between data\n6\n\n--- Page 7 ---\nFigure 2. Distribution of gaze velocities (in pixels/second)\nretention and tracking accuracy, preserving eye-tracking points while excluding clear\nmeasurement errors. Target success rates use temporal matching only, pairing targets\nchronologically with subsequent correct clicks within the reaction time window without\nspatial distance validation.\n3.3. Gaze Classification Analysis\nThe level used all validated parameters from the previous level to understand the\nstudent\u2019s behavior throughout the gameplay. This level had four main steps: fixa-\ntion/saccade detection [24], object timeline analysis [26], click-object matching, and\nperformance metrics. First, we classified the students\u2019 eye movements by examining the\nspeed of their eye movements at each moment. Next, we created a timeline of all the\nevents that occurred during gameplay by reviewing the data and marking the instances\nwhen objects appeared on the screen, disappeared, and when the student clicked on\nthem. We then arranged these events in chronological order to visualize the sequence of\nevents. After that, we matched up clicks with objects by looking at each time an object\nappeared and checking if the student clicked somewhere nearby within a reasonable\namount of time; we used our validated settings (522-5000 milliseconds for timing) to\ndecide if a click was meant for a specific object or if it was just random. Finally, we cal-\nculated performance metrics by counting the number of targets the student successfully\nclicked, the number of mistakes they made on distractors, the speed of their reactions,\nand the accuracy of their clicks, providing us with numbers that indicated how well the\nstudent was paying attention and responding during gameplay.\nAfter analyzing the data, we examined two scenarios: the first scenario presented\nthe overall analysis, providing a detailed breakdown of each student, and the multilevel\nanalysis compared the performance of each student across all three levels. We create\n7\n\n--- Page 8 ---\nfour different types of visual displays: (1) a timeline chart showing exactly when objects\nappeared and when clicks happened, (2) an eye movement pattern map showing where\nthe student looked and how their eyes moved around the screen, (3) a velocity graph\nshowing when their eyes were moving fast or slow over time, an attention heatmap\nwith colored areas showing where they spent the most time looking plus markers for\ntheir clicks, and (4) a performance dashboard with charts showing their success rates\nand reaction times. For multilevel analysis, we created comparison charts that show\nhow students\u2019 performance changed from one level to the following, trend graphs that\nindicate whether they improved or worsened over time, and summary tables that display\nall the essential numbers side by side for easy comparison.\n4. VisiTrail Analysis and Results\nThis section demonstrates the results of the proposed tool. Figs. 3, 4 depict the Graph-\nical User Interface (GUI) of the proposed VisiTrail Tool. The GUI initially displays the\nexpected data format and predefined parameters. Further, it asks the user to input the\nattention game data. After inputting the data, it generates 5 tables: the first three for\nindividual game levels, the 4th one for the overall comparison, and the 5th one for the\nrecommendation. For this study, we focus on only one student\u2019s data (Student 8), as\nthe purpose was to gain insight into the student\u2019s behavior.\nFig. 5 provides a detailed chronological overview of a user\u2019s performance in an atten-\ntion game. The Y-axis categorizes the objects into three types: Mushrooms (the target),\nBlue Flowers, and Yellow-Purple Flowers (both distractors), while the X-axis represents\nthe timeline of the task, spanning approximately 196.7 seconds at the first level of the\ngameplay. Each object\u2019s appearance is marked by a horizontal bar, with green bars\nindicating the presence of target objects and red or grey bars representing distractors.\nVertical green lines signify when target objects appear, and green stars labeled \u201dCorrect\nClick\u201d indicate moments when the participant successfully identified and clicked on a\ntarget. In contrast, a single red star labeled \u201dIncorrect Click\u201d appears around 3 times,\nhighlighting the only instance where the participant mistakenly clicked on a distractor.\nDuring the gameplay, the correct click is annotated with a yellow box displaying the\nreaction time (RT) in milliseconds, which shows the time it took the user to respond\nafter the target appeared. Reaction times vary considerably, ranging from as fast as 566\nmilliseconds to around 3900 milliseconds, suggesting variability in attention or response\nreadiness. Over time, RTs improve, indicating potential adaptation or learning as the\ntask progresses. Despite frequent and evenly spaced appearances of distractors, the par-\nticipant avoids nearly all of them, making just one incorrect click, which demonstrates\nstrong attentional control and task focus. In total, 16 target objects appear during\nthe task, and the user correctly clicks on most of them, reflecting a very high level of\naccuracy. Each target object remains visible for about 0.8 seconds, during which the\nparticipant must respond quickly to register a hit. Distractors appear more often and\nlast for similar durations, adding to the challenge of the task. The single mistake and a\nfew slower reactions show some occasional lapses, but these do not significantly affect\nthe overall firm performance.\nFig. 6 presents the eye movement patterns, where it can be observed that students\nmostly looked into the area of interest (AoI). A total of 149 fixations were recorded,\nsuggesting relatively active visual scanning behavior. Additionally, 33 Saccades were\nrecorded, which typically depict short movements; their presence between fixation points\nindicates natural eye movement dynamics. Large yellow stars indicate 16 correct clicks,\n8\n\n--- Page 9 ---\nFigure 3. GUI of the VisiTrail (Sample 1)\nFigure 4. GUI of the VisiTrail (Sample 2)\nsuggesting that the user accurately identified and selected target elements on the screen.\nThese correct clicks are organized into two main clusters, one on the left side and another\non the right side of the screen, indicating where the targets were likely located. A single\nred X represents the only incorrect click, positioned near a group of correct clicks, which\nmay indicate a near-miss error. The total duration of the task was 196.7 seconds, during\nwhich 167 gaze samples were recorded. The user engaged with only 4% of the screen\narea, suggesting that their focus was on specific regions rather than the entire interface.\nOverall, the user demonstrated a high level of accuracy, successfully selecting 15 out of\n16 targets, corresponding to an accuracy rate of approximately 93.8%.\nFig. 7 illustrates the gaze velocity of students during Level 1 gameplay. The x-axis\nrepresents time in milliseconds, while the y-axis shows velocity in pixels per second,\nreflecting how quickly a user\u2019s gaze moves across the screen. The blue line tracks gaze\nvelocity, capturing fluctuations in eye movement speed from moment to moment. Green-\nshaded areas indicate fixation periods, during which eye movement slows below a vali-\ndated threshold, suggesting that the user is likely focusing on a specific area. In contrast,\n9\n\n--- Page 10 ---\nFigure 5. Temporal Analysis of Eye Tracking During Gameplay. The y-axis lists object types (targets and\ndistractors), and the x-axis shows the task timeline. Green bars and lines indicate target appearances and\ncorrect clicks, while red/grey bars mark distractors. Red stars denote incorrect clicks on distractors.\nFigure 6. Eye Movement pattern Analysis. The pink dots indicate the saccades (rapid eye movements between\nfixations), and the light green dots represent the fixations made by the students. The yellow stars indicate\ncorrect clicks, and the red cross indicates incorrect clicks.\nred-shaded regions highlight saccadic movements, where the eye rapidly shifts between\ndifferent points of interest. A red dashed line marks the fixation/saccade threshold at\n721 pixels per second, distinguishing between slower fixation movements and faster sac-\ncades. Peaks in the velocity curve that exceed this line are interpreted as saccades.\nDuring the session, the peak velocity reached 1643 pixels per second, while the average\n10\n\n--- Page 11 ---\nvelocity was 297.1 pixels per second. The fixation rate was 89.2%, indicating that most\nof the session was spent in visual fixation rather than scanning. Additionally, vertical\ndashed lines represent different click events, such as \u2019Correct\u2019, \u2019Incorrect\u2019, or \u2019Neutral\u2019 .\nFigure 7. Eye Speed Over Time. The x-axis shows time (ms) and the y-axis shows gaze velocity (px/s). The\nblue line represents gaze velocity over time. Green areas indicate fixations (slow eye movement), red areas show\nsaccades (rapid shifts), and the red dashed line marks the threshold distinguishing the two.\nFig. 8 provides a comprehensive overview of the students\u2019 target detection accuracy,\nreaction speed, gaze movement, and movement frequency at level 1 of the gameplay. In\nthe top-left pie chart, the student demonstrated perfect accuracy by hitting 15 targets,\nresulting in 93.8% hits and 6.2% misses. The second graph (top-right bar graph) shows\nthat the majority of the students\u2019 reaction times were between 400 and 550 millisec-\nonds, with a calculated average of 517 ms. This suggests consistent and relatively quick\nresponses. The 3rd graph, bottom-left pie chart, reveals that 89.2% of the user\u2019s eye\nmovements were fixations, steady visual focus, while only 10.8% were saccades, which\nare rapid eye movements between points. Lastly, the fourth graph, bottom-right bar\ngraph, visually reinforces this finding, showing that fixations occurred more than 140\ntimes, whereas saccades were recorded only about 20 times. Together, these graphs in-\ndicate not only high task accuracy and efficient response times but also a gaze pattern\ndominated by stable visual attention.\nFig. 9 depicts 4 graphs. The first graph compares the success rates, the second graph\nshows the response times, the third graph displays the total screen area used, and the\nfourth graph illustrates the number of mistakes made. It can be seen that the highest\ntarget hit rate was achieved at the 2nd level, and the lowest was at the 1st level. The\nhighest time response time was on level 1, and the lowest was on level 2. Furthermore,\nthe student occupies the most screen area at level 1 and is the weakest on level 3.\nFinally, the student made most of the mistakes in level 3.\nThe results reveal a clear picture of this student\u2019s attention and learning patterns.\nIn level 1, they struggled a little, finding around 94% of targets with slow responses\n11\n\n--- Page 12 ---\nFigure 8. Performance Summary\n(517 ms), indicating that they were still learning the task. Levels 2 and 3 are the best\nperformers, with a 100% success rate and significantly faster responses (480 ms at Level\n2), demonstrating their ability to perform well when focused. However, level 3 showed\na concerning increase in response time to 529 ms, suggesting that the participant may\nhave become tired, overconfident, or perhaps the task had become too complex. The\nstudent also spent less time on the screen over time (from 41% to 15.6%), which indicates\nthat the student was highly focused on the AoI area. It can also be observed that the\nnumber of mistakes made by students increases as the level of difficulty increases.\nFor psychologists, this data provides objective evidence of attention patterns that\nthey can precisely observe when attention peaks and declines, measure how long the\nstudent can maintain focus, and track whether interventions or medications are effective\nby comparing these numbers over time. Teachers can use this information to time their\nmost important lessons during the student\u2019s peak attention period (such as level 2), plan\nbreaks before fatigue sets in (before level 3 declines), and recognize that this student\nrequires extra support with impulse control, as their mistake rate increases dramatically\nwhen they are tired. Both professionals can work together using these concrete numbers\nto create personalized strategies that align with the student\u2019s natural attention rhythms.\n5. Conclusion and Future Work\nThis paper presented a time-series analysis tool that combines multiple analytical di-\nmensions: enhanced movement classification that distinguishes between fixations, sac-\ncades, and smooth pursuit; temporal pattern analysis that reveals how attention evolves\nthroughout task performance; object-click sequence tracking that directly links visual\n12\n\n--- Page 13 ---\nFigure 9. Multilevel Performance Comparison\nattention to user actions; and performance metrics that quantify both accuracy and ef-\nficiency of visual search behavior. Psychologists and teachers can collaborate using this\ntool to create personalized strategies that align with the student\u2019s natural attention\nrhythms. In the future, we plan to design Large Language Models (LLMs) to enhance\nthis eye-tracking tool by making it more intelligent and user-friendly (e.g., LLMs can\nhelp automatically label gaze patterns, generate easy-to-understand summaries of user\nbehavior, and allow researchers to interact with the tool using simple language com-\nmands. This means users could ask questions like \u201dWhen was the user most distracted?\u201d\nand get helpful insights without needing complex code or manual analysis.)\n6. Acknowledgment\nThe research leading to these results is within the frame of the \u201dEMPOWER. Design and\nevaluation of technological support tools to empower stakeholders in digital education\u201d\nproject, which has received funding from the European Union\u2019s Horizon Europe program\nunder grant agreement No 101060918. Views and opinions expressed are, however, those\nof the author(s) only and do not necessarily reflect those of the European Union. Neither\nthe European Union nor the granting authority can be held responsible for them.\n13\n\n--- Page 14 ---\nReferences\n[1] Rehman A, Heldal I, Stilwell D, et al. Towards a supporting framework for neuro-\ndevelopmental disorder: Considering artificial intelligence, serious games and eye tracking.\nIn: 2024 IEEE International Conference on Big Data (BigData); IEEE; 2024. p. 8238\u2013\n8240.\n[2] Nov\u00b4 ak J \u02c7S, Masner J, Benda P, et al. Eye tracking, usability, and user experience: A sys-\ntematic review. International Journal of Human\u2013Computer Interaction. 2024;40(17):4484\u2013\n4500.\n[3] Ali Q, Heldal I, Helgesen CG. Towards a framework for visualization and analysis of\neye tracking data for functional vision screening. In: Proceedings of the 3rd International\nHealth Data Workshop (HEDA 2023); (CEUR Workshop Proceedings; Vol. 3440). CEUR-\nWS.org; 2023. p. 3\u201312. Available from: https://ceur-ws.org/Vol-3440/paper3.pdf .\n[4] D\u00e6hlen A, Heldal I, Rehman A, et al. Towards more accurate help: Informing teachers how\nto support ndd children by serious games and eye tracking technologies. In: Proceedings\nof the 2024 Symposium on Eye Tracking Research and Applications; 2024. p. 1\u20137.\n[5] Costescu C, David C, Ros ,an A, et al. Mushroom hunters: A digital game for assessing\nand training sustained attention in children with neurodevelopmental disorders. In: Inter-\nnational Conference in Methodologies and intelligent Systems for Techhnology Enhanced\nLearning; Springer; 2023. p. 78\u201386.\n[6] L\u00a8 ams\u00a8 a J, Kotkajuuri J, Lehtinen A, et al. The focus and timing of gaze matters: In-\nvestigating collaborative knowledge construction in a simulation-based environment by\ncombined video and eye tracking. In: Frontiers in education; Vol. 7; Frontiers Media SA;\n2022. p. 942224.\n[7] Keshava A, Nezami FN, Neumann H, et al. Just-in-time: Gaze guidance in natural be-\nhavior. PLOS Computational Biology. 2024;20(10):e1012529.\n[8] Papavlasopoulou S, Sharma K, Melhart D, et al. Investigating gaze interaction to sup-\nport children\u2019s gameplay. International Journal of Child-Computer Interaction. 2021;\n30:100349.\n[9] Frutos-Pascual M, Garcia-Zapirain B. Assessing visual attention using eye tracking sensors\nin intelligent cognitive therapies based on serious games. Sensors. 2015;15(5):11092\u201311117.\n[10] Piazzalunga C, Dui LG, Termine C, et al. Investigating visual perception impairments\nthrough serious games and eye tracking to anticipate handwriting difficulties. Sensors.\n2023;23(4):1765.\n[11] Velichkovsky BB, Khromov N, Korotin A, et al. Visual fixations duration as an indicator\nof skill level in esports. In: Human-Computer Interaction\u2013INTERACT 2019: 17th IFIP\nTC 13 International Conference, Paphos, Cyprus, September 2\u20136, 2019, Proceedings, Part\nI 17; Springer; 2019. p. 397\u2013405.\n[12] Kim M, Lee J, Lee SY, et al. Development of an eye-tracking system based on a deep\nlearning model to assess executive function in patients with mental illnesses. Scientific\nReports. 2024;14(1):18186.\n[13] Argasi\u00b4 nski JK, Grabska-Gradzi\u00b4 nska I. Patterns in serious game design and evaluation\napplication of eye-tracker and biosensors. In: International Conference on Artificial Intel-\nligence and Soft Computing; Springer; 2017. p. 367\u2013377.\n[14] Hajari N, He W, Cheng I, et al. Spatio-temporal eye gaze data analysis to better under-\nstand team cognition. In: Smart Multimedia: First International Conference, ICSM 2018,\nToulon, France, August 24\u201326, 2018, Revised Selected Papers 1; Springer; 2018. p. 39\u201348.\n[15] Du L, Jia J, Zhang X, et al. Privategaze: Preserving user privacy in black-box mobile\ngaze tracking services. Proceedings of the ACM on Interactive, Mobile, Wearable and\nUbiquitous Technologies. 2024;8(3):1\u201328.\n[16] Paskovske A, Kliziene I. Eye tracking technology on children\u2019s mathematical educa-\ntion: systematic review. In: Frontiers in Education; Vol. 9; Frontiers Media SA; 2024.\np. 1386487.\n[17] Bueno ML, Thill S. Datasets for artificial intelligence in education: The case of children\n14\n\n--- Page 15 ---\nwith neurodevelopmental disorders. In: International Conference in Methodologies and\nintelligent Systems for Techhnology Enhanced Learning; Springer; 2023. p. 70\u201377.\n[18] Thill S, Charisi V, Belpaeme T, et al. From modelling to understanding children\u2019s\nbehaviour in the context of robotics and social artificial intelligence. arXiv preprint\narXiv:221011161. 2022;.\n[19] Costescu C, Rosan A, Petru A, et al. Development of a technological screening platform for\nchildren. In: 2020 11th IEEE International Conference on Cognitive Infocommunications\n(CogInfoCom); IEEE; 2020. p. 000453\u2013000458.\n[20] Salvucci DD, Goldberg JH. Identifying fixations and saccades in eye-tracking protocols.\nIn: Proceedings of the Symposium on Eye Tracking Research & Applications (ETRA);\n2000. p. 71\u201378.\n[21] Olsen A. The tobii i-vt fixation filter. Tobii Technology. 2012;21(4-19):5.\n[22] PhD JFS, MSc MST, MSc DISW, et al. Sustained attention and executive func-\ntioning performance in attention-deficit/hyperactivity disorder. Child Neuropsychol-\nogy. 2005;11(3):285\u2013294. PMID: 16036452; Available from: https://doi.org/10.1080/\n09297040490916938 .\n[23] Holmqvist K, Nystr\u00a8 om M, Andersson R, et al. Eye tracking: A comprehensive guide to\nmethods and measures. oup Oxford; 2011.\n[24] Henderson JM. Human gaze control during real-world scene perception. Trends in Cog-\nnitive Sciences. 2003;7(11):498\u2013504.\n[25] Nystr\u00a8 om M, Andersson R, Holmqvist K, et al. The influence of calibration method and eye\nphysiology on eyetracking data quality. Behavior Research Methods. 2010;42(1):188\u2013204.\n[26] Jarodzka H, Holmqvist K, Gruber H. Eye tracking and education: A selective review of\nthe literature. Educational Psychology Review. 2010;22(2):123\u2013146.\n15",
  "project_dir": "artifacts/projects/enhanced_cs.HC_2508.20522v1_VisiTrail_A_Cognitive_Visualization_Tool_for_Time",
  "communication_dir": "artifacts/projects/enhanced_cs.HC_2508.20522v1_VisiTrail_A_Cognitive_Visualization_Tool_for_Time/.agent_comm",
  "assigned_at": "2025-08-31T20:58:51.491307",
  "status": "assigned"
}